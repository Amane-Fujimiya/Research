{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Unattainable AGI and why it is not here\n",
        "\n",
        "For a long time, there exists the debate on whether the world is\n",
        "approaching the fundamental point of “Singularity”, as per the creation\n",
        "of an artificial general intelligence (AGI) in various forms, both in\n",
        "popular social phenomena and in research society. Just how much is this\n",
        "true?\n",
        "\n",
        "> The research is in progress, so don’t ask me why (草)\n",
        "\n",
        "### Summary\n",
        "\n",
        "Large Language Model, or LLM ((Vaswani et al. 2017),(Devlin et al.\n",
        "2019),(Brown et al. 2020),(Zhao et al. 2023),(Zhao et al. 2024),(Radford\n",
        "et al. 2019),(Radford et al. 2018),(Raffel et al. 2020),(Touvron,\n",
        "Lavril, Izacard, Martinet, Lachaux, Lacroix, Rozière, Goyal, Hambro,\n",
        "Azhar, and others 2023),(Touvron, Martin, et al. 2023),(Aakanksha\n",
        "Chowdhery et al. 2022),(Ouyang et al. 2022),(Wei et al. 2022),(Kaplan et\n",
        "al. 2020),(Hoffmann et al. 2022),(Bai et al. 2022)) is one of the most\n",
        "successful, most advanced, and most developed type of model in the\n",
        "current modern machine learning landscape, and of AI (Artificial\n",
        "Intelligence) research at large. Its success has not been lacking, and\n",
        "its reputation and widespread uses have been proved over time. The\n",
        "effect of LLM has been realized, and indeed has been changing the\n",
        "landscape of society in a very much difficult way. Latest model of such\n",
        "architecture, like (OpenAI 2025b),(Wang et al. 2025),(OpenAI 2025a)’s\n",
        "GPT-5, (Guo et al. 2025),(DeepSeek AI / Hugging Face 2025)’s DeepSeek\n",
        "AI, (Anthropic 2024),(Anthropic Research 2025)’s Claude AI, (Touvron,\n",
        "Lavril, Izacard, Martinet, Lachaux, Lacroix, Rozière, Goyal, Hambro,\n",
        "Azhar, Rodriguez, et al. 2023)’s LLaMA, (Amitabh Chowdhery et al.\n",
        "2022),(Anil et al. 2023)’s *PaLM / PaLM-2*, and (Jiang et al.\n",
        "2023),(Mistral AI 2023)’s Mistral (Mistral-7B), pushed this boundary\n",
        "further and further, and levelling up many tasks and purposes with AI\n",
        "system in practice, and further onward with techniques like (Wei et al.\n",
        "2022)’s *Chain-of-Thought prompting*, (Kaplan et al. 2020)’s scaling\n",
        "law, and more ((Hoffmann et al. 2022),(Bai et al. 2022)).\n",
        "\n",
        "However, with such development, come great expectation, great\n",
        "speculation, and also great hallucination. New development of the field\n",
        "of AI even earlier than paper on the Transformer neural network which\n",
        "fuelled the revolution of AI exposure, has gathered a group of people\n",
        "speculated about the further exponential growth of AI, almost to a\n",
        "degree of religious, about the topic of a *Singularity*, where AI will\n",
        "become **Artificial General Intelligence** (AGI). This is reflected in\n",
        "popular culture phenomena, speculation, researches, interpretation of\n",
        "reasoning behaviours and so on, for example, in (Barrat 2013),(Birch\n",
        "2024),(Yudkowsky and Soares 2025),(Hao 2025),(Bostrom 2014), and more\n",
        "broadly on LLM in specific, (Mumuni and Mumuni 2025),(Shang et al.\n",
        "2024),(David Ilić 2024),(Goertzel 2023),(Feng et al. 2024),(Ryunosuke\n",
        "Ishizaki 2025) and (Cui et al. 2024). The claim is clear - we are\n",
        "pushing toward the age of AGI, and perhaps sooner or later, reach the\n",
        "state of Artificial Superintelligence (ASI) of which cited in popular\n",
        "culture as the cultivation point of the Singularity - the shift of\n",
        "society toward a society of abundance, post-scarcity state. Most\n",
        "proponents point to LLM for such advancement, as it is one of the most\n",
        "widespread success and accessible form of interaction with AI systems on\n",
        "large scale. Pushbacks against such movement, including such as\n",
        "(Friedman 2024),(Anonymous 2024c),(Bender et al. 2021),(Baan\n",
        "2021),(Anonymous 2023) on the “Stochastic Parrot Hypothesis”, (Anonymous\n",
        "2024a) questioned of LLM path to AGI, the reverend (Forum 2025) post\n",
        "itself, (Anonymous 2024b),(Anonymous 2025) on generating suggestive\n",
        "limitation of research paper, (Marcus 2025),(Marcus 2023)‘s’ critique on\n",
        "generative AI on world models and failure of LLM, Sandra Johnson (2024)\n",
        "on limitation of LLM, similarly (Zhang et al. 2025), mathematics\n",
        "critique in (Mirzadeh et al. 2024), and more. However, the generally\n",
        "public, and more so of the positivity of the inner market on AI focus on\n",
        "the development and increment of larger models toward such goal. It is\n",
        "not too offset to hear the phrase “AGI will be in $X$ days/month/year”,\n",
        "as much as it is a social phenomena even in small or large circle.\n",
        "Objectively, such positivity is not without basis. Furthermore, it is\n",
        "rather with certain amount of irony that the research made use of AI\n",
        "itself, for reference taking purposes.\n",
        "\n",
        "Nevertheless, a critical task can be given out of such argument and\n",
        "thorough development of the current debate. What can then be\n",
        "extrapolated from the ongoing dilemma? What has to do with the\n",
        "architecture, the consideration about AGI that is now turned into the\n",
        "debate of will LLM be AGI? How is our understanding of the concept of\n",
        "AI, AGI, and ASI in general? And of a sense, what will provide us a\n",
        "pathway toward such goal?\n",
        "\n",
        "Anil, R. et al. 2023. “PaLM 2 Technical Report.” arXiv preprint\n",
        "arXiv:2305.10403. <https://arxiv.org/abs/2305.10403>.\n",
        "\n",
        "Anonymous. 2023. “Stochastic Parrots: A Novel Look at Large Language\n",
        "Models and Their Limitations.” Towards AI.\n",
        "<https://towardsai.net/p/machine-learning/stochastic-parrots-a-novel-look-at-large-language-models-and-their-limitations>.\n",
        "\n",
        "———. 2024a. “How Close Is AGI Actually? Why LLMs Alone Will Not Get Us\n",
        "to AGI.” New Jersey Innovation Institute.\n",
        "<https://www.njii.com/2024/07/why-llms-alone-will-not-get-us-to-agi/>.\n",
        "\n",
        "———. 2024b. “LimGen: Probing the LLMs for Generating Suggestive\n",
        "Limitations of Research Papers.” *arXiv Preprint arXiv:2403.15529*.\n",
        "<https://arxiv.org/abs/2403.15529>.\n",
        "\n",
        "———. 2024c. “The Stochastic Parrot Hypothesis Is Debatable for the Last\n",
        "Generation of LLMs.”\n",
        "<https://www.lesswrong.com/posts/HxRjHq3QG8vcYy4yy/the-stochastic-parrot-hypothesis-is-debatable-for-the-last>.\n",
        "\n",
        "———. 2025. “Can LLMs Identify Critical Limitations Within Scientific\n",
        "Research? A Systematic Evaluation on AI Research Papers.” *arXiv\n",
        "Preprint arXiv:2507.02694*. <https://arxiv.org/abs/2507.02694>.\n",
        "\n",
        "Anthropic. 2024. “The Claude 3 Model Family: Opus, Sonnet, Haiku (Model\n",
        "Card).” <https://www-cdn.anthropic.com/.../Model_Card_Claude_3.pdf>.\n",
        "\n",
        "Anthropic Research. 2025. “Tracing the Thoughts of a Large Language\n",
        "Model.”\n",
        "<https://www.anthropic.com/research/tracing-thoughts-language-model>.\n",
        "\n",
        "Baan, Joris. 2021. “The Slodderwetenschap (Sloppy Science) of Stochastic\n",
        "Parrots – a Plea for Science to NOT Take the Route Advocated by Gebru\n",
        "and Bender.” *arXiv Preprint arXiv:2101.10098*.\n",
        "<https://arxiv.org/abs/2101.10098>.\n",
        "\n",
        "Bai, Yuntao, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson\n",
        "Kernion, Andy Jones, Anna Chen, et al. 2022. “Constitutional AI:\n",
        "Harmlessness from AI Feedback.” *arXiv Preprint arXiv:2212.08073*.\n",
        "<https://arxiv.org/abs/2212.08073>.\n",
        "\n",
        "Barrat, James. 2013. *Our Final Invention: Artificial Intelligence and\n",
        "the End of the Human Era*. Thomas Dunne Books.\n",
        "\n",
        "Bender, Emily M, Timnit Gebru, Angelina McMillan-Major, and Shmargaret\n",
        "Shmitchell. 2021. “On the Dangers of Stochastic Parrots: Can Language\n",
        "Models Be Too Big?” In *Proceedings of the 2021 ACM Conference on\n",
        "Fairness, Accountability, and Transparency*, 610–23. ACM.\n",
        "<https://doi.org/10.1145/3442188.3445922>.\n",
        "\n",
        "Birch, Jonathan. 2024. *The Edge of Sentience: Risk and Precaution in\n",
        "Humans, Other Animals, and AI*. Oxford University Press.\n",
        "\n",
        "Bostrom, Nick. 2014. *Superintelligence: Paths, Dangers, Strategies*.\n",
        "Oxford University Press.\n",
        "\n",
        "Brown, Tom, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,\n",
        "Prafulla Dhariwal, Arvind Neelakantan, et al. 2020. “Language Models Are\n",
        "Few-Shot Learners.” In *Advances in Neural Information Processing\n",
        "Systems*, 33:1877–1901. Neural Information Processing Systems.\n",
        "<https://arxiv.org/abs/2005.14165>.\n",
        "\n",
        "Chowdhery, Aakanksha, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav\n",
        "Mishra, Adam Roberts, Paul Barham, et al. 2022. “PaLM: Scaling Language\n",
        "Modeling with Pathways.” *arXiv Preprint arXiv:2204.02311*.\n",
        "<https://arxiv.org/abs/2204.02311>.\n",
        "\n",
        "Chowdhery, Amitabh et al. 2022. “PaLM: Scaling Language Modeling with\n",
        "Pathways.” In. arXiv preprint arXiv:2204.02311.\n",
        "<https://arxiv.org/abs/2204.02311>.\n",
        "\n",
        "Cui, Tianyu, Yanling Wang, Chuanpu Fu, Yong Xiao, Sijia Li, Xinhao Deng,\n",
        "Yunpeng Liu, et al. 2024. “Risk Taxonomy, Mitigation, and Assessment\n",
        "Benchmarks of Large Language Model Systems.” *arXiv Preprint*.\n",
        "<https://arxiv.org/abs/2401.05778>.\n",
        "\n",
        "David Ilić, Gilles E. Gignac. 2024. “Evidence of Interrelated\n",
        "Cognitive-Like Capabilities in Large Language Models: Indications of\n",
        "Artificial General Intelligence or Achievement?” *Intelligence* 106:\n",
        "101858. <https://doi.org/10.1016/j.intell.2024.101858>.\n",
        "\n",
        "DeepSeek AI / Hugging Face. 2025. “Deepseek-Ai/DeepSeek-R1 (Model\n",
        "Card).” <https://huggingface.co/deepseek-ai/DeepSeek-R1>.\n",
        "\n",
        "Devlin, Jacob, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019.\n",
        "“BERT: Pre-Training of Deep Bidirectional Transformers for Language\n",
        "Understanding.” In *Proceedings of the 2019 Conference of the North\n",
        "American Chapter of the Association for Computational Linguistics: Human\n",
        "Language Technologies, Volume 1 (Long and Short Papers)*, 4171–86.\n",
        "Association for Computational Linguistics.\n",
        "<https://arxiv.org/abs/1810.04805>.\n",
        "\n",
        "Feng, Tao, Chuanyang Jin, Jingyu Liu, Kunlun Zhu, Haoqin Tu, Zirui\n",
        "Cheng, Guanyu Lin, and Jiaxuan You. 2024. “How Far Are We from AGI: Are\n",
        "LLMs All We Need?” *arXiv Preprint arXiv:2405.10313*, May.\n",
        "<https://arxiv.org/abs/2405.10313>.\n",
        "\n",
        "Forum, AI Alignment. 2025. “Beware General Claims about \"Generalizable\n",
        "Reasoning Capabilities\" (of Modern AI Systems).” AI Alignment Forum.\n",
        "<https://www.alignmentforum.org/posts/5uw26uDdFbFQgKzih/beware-general-claims-about-generalizable-reasoning>.\n",
        "\n",
        "Friedman, Dave. 2024. “Understanding Inference and the \"Stochastic\n",
        "Parrot\" in Large Language Models.” Personal blog (Substack).\n",
        "<https://davefriedman.substack.com/p/understanding-inference-and-the-stochastic>.\n",
        "\n",
        "Goertzel, Ben. 2023. “Generative AI Vs. AGI: The Cognitive Strengths and\n",
        "Weaknesses of Modern LLMs.” *arXiv Preprint*.\n",
        "<https://arxiv.org/abs/2309.10371>.\n",
        "\n",
        "Guo, D. et al. 2025. “DeepSeek-R1: Incentivizing Reasoning Capability in\n",
        "LLMs.” arXiv preprint arXiv:2501.12948.\n",
        "<https://arxiv.org/abs/2501.12948>.\n",
        "\n",
        "Hao, Karen. 2025. *Empire of AI: Dreams and Nightmares in Sam Altman’s\n",
        "OpenAI*. Penguin Press.\n",
        "\n",
        "Hoffmann, Jordan, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya,\n",
        "Trevor Cai, Eliza Rutherford, Diego de Las Casas, et al. 2022. “Training\n",
        "Compute-Optimal Large Language Models.” *arXiv Preprint\n",
        "arXiv:2203.15556*. <https://arxiv.org/abs/2203.15556>.\n",
        "\n",
        "Jiang, A. Q. et al. 2023. “Mistral 7B.” arXiv preprint arXiv:2310.06825.\n",
        "<https://arxiv.org/abs/2310.06825>.\n",
        "\n",
        "Kaplan, Jared, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin\n",
        "Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario\n",
        "Amodei. 2020. “Scaling Laws for Neural Language Models.” *arXiv Preprint\n",
        "arXiv:2001.08361*. <https://arxiv.org/abs/2001.08361>.\n",
        "\n",
        "Marcus, Gary. 2023. “Elegant and Powerful New Result That Seriously\n",
        "Undermines Large Language Models.” Marcus on AI (Substack).\n",
        "<https://garymarcus.substack.com/p/elegant-and-powerful-new-result-that>.\n",
        "\n",
        "———. 2025. “A Knockout Blow for LLMs?” *Communications of the ACM*.\n",
        "<https://cacm.acm.org/blogcacm/a-knockout-blow-for-llms/>.\n",
        "\n",
        "Mirzadeh, Iman, Keivan Alizadeh, Hooman Shahrokhi, Oncel Tuzel, Samy\n",
        "Bengio, and Mehrdad Farajtabar. 2024. “GSM-Symbolic: Understanding the\n",
        "Limitations of Mathematical Reasoning in Large Language Models.” *arXiv\n",
        "Preprint arXiv:2410.05229*. <https://arxiv.org/abs/2410.05229>.\n",
        "\n",
        "Mistral AI. 2023. “Announcing Mistral 7B.”\n",
        "<https://mistral.ai/news/announcing-mistral-7b>.\n",
        "\n",
        "Mumuni, Alhassan, and Fuseini Mumuni. 2025. “Large Language Models for\n",
        "Artificial General Intelligence: A Survey of Foundational Principles and\n",
        "Approaches.” *arXiv Preprint*. <https://arxiv.org/abs/2501.03151>.\n",
        "\n",
        "OpenAI. 2025a. “Inside GPT-5 for Work.”\n",
        "<https://cdn.openai.com/pdf/inside-gpt-5-for-work.pdf>.\n",
        "\n",
        "———. 2025b. “Introducing GPT-5.” <https://openai.com/gpt-5/>.\n",
        "\n",
        "Ouyang, Long, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright,\n",
        "Pamela Mishkin, Chong Zhang, et al. 2022. “Training Language Models to\n",
        "Follow Instructions with Human Feedback.” *Advances in Neural\n",
        "Information Processing Systems* 35: 27730–44.\n",
        "<https://arxiv.org/abs/2203.02155>.\n",
        "\n",
        "Radford, Alec, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever.\n",
        "2018. “Improving Language Understanding by Generative Pre-Training.”\n",
        "*OpenAI Blog*.\n",
        "<https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf>.\n",
        "\n",
        "Radford, Alec, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and\n",
        "Ilya Sutskever. 2019. “Language Models Are Unsupervised Multitask\n",
        "Learners.” *OpenAI Blog* 1 (8): 9.\n",
        "<https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf>.\n",
        "\n",
        "Raffel, Colin, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang,\n",
        "Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. “Exploring\n",
        "the Limits of Transfer Learning with a Unified Text-to-Text\n",
        "Transformer.” *Journal of Machine Learning Research* 21 (140): 1–67.\n",
        "<https://arxiv.org/abs/1910.10683>.\n",
        "\n",
        "Ryunosuke Ishizaki, Mahito Sugiyama. 2025. “Large Language Models:\n",
        "Assessment for Singularity.” *AI & Society*.\n",
        "<https://link.springer.com/article/10.1007/s00146-025-02271-4>.\n",
        "\n",
        "Sandra Johnson, David Hyland-Wood. 2024. “A Primer on Large Language\n",
        "Models and Their Limitations.” *arXiv Preprint arXiv:2412.04503*.\n",
        "<https://arxiv.org/abs/2412.04503>.\n",
        "\n",
        "Shang, Jingbo, Zai Zheng, Jiale Wei, Xiang Ying, Felix Tao, and\n",
        "Mindverse Team. 2024. “AI-Native Memory: A Pathway from LLMs Towards\n",
        "AGI.” *arXiv Preprint*. <https://arxiv.org/abs/2406.18312>.\n",
        "\n",
        "Touvron, Hugo, Thibaut Lavril, Gautier Izacard, Xavier Martinet,\n",
        "Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,\n",
        "Eric Hambro, Faisal Azhar, and others. 2023. “LLaMA: Open and Efficient\n",
        "Foundation Language Models.” *arXiv Preprint arXiv:2302.13971*.\n",
        "<https://arxiv.org/abs/2302.13971>.\n",
        "\n",
        "Touvron, Hugo, Thibaut Lavril, Gautier Izacard, Xavier Martinet,\n",
        "Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,\n",
        "Eric Hambro, Faisal Azhar, Aurélien Rodriguez, et al. 2023. “LLaMA: Open\n",
        "and Efficient Foundation Language Models.” *arXiv Preprint\n",
        "arXiv:2302.13971*. <https://arxiv.org/abs/2302.13971>.\n",
        "\n",
        "Touvron, Hugo, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi,\n",
        "Yasmine Babaei, Nikolay Bashlykov, et al. 2023. “Llama 2: Open\n",
        "Foundation and Fine-Tuned Chat Models.” *arXiv Preprint\n",
        "arXiv:2307.09288*. <https://arxiv.org/abs/2307.09288>.\n",
        "\n",
        "Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion\n",
        "Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017.\n",
        "“Attention Is All You Need.” In *Advances in Neural Information\n",
        "Processing Systems*, 30:5998–6008. Neural Information Processing\n",
        "Systems. <https://arxiv.org/abs/1706.03762>.\n",
        "\n",
        "Wang, Shansong, Mingzhe Hu, Qiang Li, Mojtaba Safari, and Xiaofeng Yang.\n",
        "2025. “Capabilities of GPT-5 on Multimodal Medical Reasoning.”\n",
        "<https://arxiv.org/abs/2508.08224>.\n",
        "\n",
        "Wei, Jason, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed\n",
        "Chi, Quoc V Le, Denny Zhou, et al. 2022. “Chain-of-Thought Prompting\n",
        "Elicits Reasoning in Large Language Models.” *Advances in Neural\n",
        "Information Processing Systems* 35: 24824–37.\n",
        "<https://arxiv.org/abs/2201.11903>.\n",
        "\n",
        "Yudkowsky, Eliezer, and Nate Soares. 2025. *If Anyone Builds It,\n",
        "Everyone Dies: Why Superhuman AI Would Kill Us All*. Hachette Book\n",
        "Group.\n",
        "\n",
        "Zhang, Yijun et al. 2025. “LLLMs: A Data-Driven Survey of Evolving\n",
        "Research on Limitations of Large Language Models.” *arXiv Preprint\n",
        "arXiv:2505.19240*, May. <https://arxiv.org/abs/2505.19240>.\n",
        "\n",
        "Zhao, Wayne Xin et al. 2024. “Large Language Models: A Survey.” *arXiv\n",
        "Preprint arXiv:2402.06196*. <https://arxiv.org/abs/2402.06196>.\n",
        "\n",
        "Zhao, Wayne Xin, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng\n",
        "Hou, Yingqian Min, et al. 2023. “A Survey of Large Language Models.”\n",
        "*arXiv Preprint arXiv:2303.18223*. <https://arxiv.org/abs/2303.18223>."
      ],
      "id": "54f4ef02-7ece-4a3c-9414-c8f264ac5ee1"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  }
}