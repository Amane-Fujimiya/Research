[
  {
    "objectID": "tutorials.html",
    "href": "tutorials.html",
    "title": "Tutorials",
    "section": "",
    "text": "We provide an extensive set of tutorials to get familiar with devito’s internals, capabilities and use cases. In details we offer the following topics:\n\nUser API contains API description notebooks to get started with Devito.\nCompiler introduces the compiler and the internal representation used to lower the symbolic expression to high performance code.\nPerformance tips introduce the performance optimization knobs exposed to the users.\nSeismic modeling and inversion contains a broad set of notebooks that implements various wave equations for modeling and inversion.\nCFD offers a full rendering of the original CFD Python: 12 steps to Navier-Stokes by Lorena Barba using Devito.\nFinance contains an introductory notebook using Devito to model the Black-Scholes equations.\n\nAll those notebooks can be interactively explored in our binder server as well.\n\n\n\n Back to top"
  },
  {
    "objectID": "sponsors.html",
    "href": "sponsors.html",
    "title": "Sponsors",
    "section": "",
    "text": "We gratefully acknowledge the support received to develop Devito from industry and research council funding.\n\nOpen source Devito consortium\nThis was a 3-year consortium that ran from January 2020-December 2022. The consortium is now closed. Please see https://www.devitocodes.com/ and contact details therein if you are interested in using Devito commercially. Consortium members are:\n\nBP\nDUG\nPetrobras\nShell\n\n\n\nOther industry sponsors\n\nAMD\nChevron\nFujitsu\nIntel\nMicrosoft\nNvidia\n\n\n\nResearch Council grants\n\nEP/W007789/1\nEP/V001493/1\nEP/R029423/1\n\n\n\nDevito Codes Ltd.\nDevito is also supported directly by Devito Codes Ltd. Devito Codes Ltd uses an open-core business model to provide commercial services while reinvesting in Devito’s open-source development, support, and maintenance.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "The Research Companionship",
    "section": "",
    "text": "Here we are, organization of the research base. For now, this site is hosted and structured on the provision and purposes of 藤宮天音, or Bui Gia Khanh, of here, or it is just me. There should be not much going on here, however, you might as well treat it as my very own hub of operation at large, in public, for research, collaborations, et cetera. Also, I hate coding, so I will need something in here… quick. The template for this site is basically, devitoproject.org, of which I take for certain specification.\nMost of the content here right now are proposals, ideas, and research problems that I took virtue to describe and partake upon. Individual articles are set up here for reference purpose as to describe the research topic at large. For project, documentation purposes fit them neatly in such realm, and for documentation on various topics, we delegate such description to their own due."
  },
  {
    "objectID": "index.html#current-topics",
    "href": "index.html#current-topics",
    "title": "The Research Companionship",
    "section": "Current topics",
    "text": "Current topics\nMy current topic of interest includes:\n\nComputational learning theory and statistical learning theory.\nDeep neural network theoretical explanation.\nFoundation of machine learning research.\nFoundation of physics.\nRaman spectroscopy and deep learning application.\nNeurology and computational models of neurophysiology.\nQuantum mechanics and quantum well (of semiconductor device structures).\n\nMost of them are theoretical, some are practical in the sense of model evaluation and thereof. Generally, I am fairly bad of practical application, though that is a front to improve upon. I am also interested in mathematics, including the theory of probability, category theory. Some of the potential topics then also includes theoretical computational model, type theory, and theoretical mathematical modelling."
  },
  {
    "objectID": "index.html#current-collaborator",
    "href": "index.html#current-collaborator",
    "title": "The Research Companionship",
    "section": "Current collaborator",
    "text": "Current collaborator\nRight, now my collaborators are Daud Shahbaz, Duong Ngoc Khoa, and a few more people. If you want to collaborate, please look into the catalogue for some of my proposals and already in progress works. Though I doubt at this stage there can be anything to offer."
  },
  {
    "objectID": "content/r29.html",
    "href": "content/r29.html",
    "title": "",
    "section": "",
    "text": "The research on this site is of the Theory of Raman Spectroscopy. Participant in such regard is Duong Ngoc Khoa and Yogesh Verma (a particular person we met during previous ASC 2025). The sheet for organization is this one. Github site will be delivered together in the long term."
  },
  {
    "objectID": "content/r29.html#planning",
    "href": "content/r29.html#planning",
    "title": "",
    "section": "Planning",
    "text": "Planning\nIt is then we have to elaborate on explaining what we are going to do within the confinement of the topic by itself. Originally, the idea comes from my own blog documentation over here of the same expository analysis toward Raman Spectroscopy. So most of them will be there, and it will not be so particularly hard to figure out what to do in such research. The goal of the research, nevertheless, is to gain a comprehensive framework and outlook toward the problem of Raman spectroscopy, what theories support and explain it in what way, caveat, concepts that is used in analysing Raman spectroscopy, what type of differences between classical and quantum mechanical treatments in vibrational infrared range, and so on so forth. This all stems from the hardship in delivering and targeting the problem itself during the process of writing that essay, so to speak."
  },
  {
    "objectID": "content/r29.html#what-is-raman-spectroscopy",
    "href": "content/r29.html#what-is-raman-spectroscopy",
    "title": "",
    "section": "What is Raman spectroscopy?",
    "text": "What is Raman spectroscopy?\n\nAccording to the traditional view of the creation, it was on the first day that light was separated. Quite soon after, our forebears developed an appetite for knowledge which their progeny have never lost. However, it is only relatively recently that they discovered that, if light is reunited fleetingly with matter and then scattered again, it carries with it detailed information – albeit in code – about the structure of matter. It is the purpose of this book to provide the means for deciphering the information carried by the scattered light. (Claude Monet)\n\nIn the same spirit, Raman scattering is the interaction resultant of photon interaction with matters, in its own sense, and of its own length of range. Predictably, redirection and changes of frequency of incident lights propagating through the medium. The scattering effect is elastic if frequency is constant, but inelastic if it varies.\nThe word spectroscopy indicates utilization of electromagnetic radiation, when interaction with atoms and molecules. Their ‘after effect’ varies, for example, being induced absorption, spontaneous emission, induced emission. Some of the more direct descriptions includes luminescence and fluorescence, scattering, and reflections. Of all, the typical interest is based on the event of scattering.\nFor a scattering coefficient \\(k_{s}\\), the intensity of light of wavelength \\(\\lambda\\) after passing through a medium of thickness \\(L\\), \\(I(\\lambda)\\), may be given by \\[\nI(\\lambda)=I_{0}(\\lambda)\\exp{(-k_{s}L)}\n\\] where \\(I_{0}(\\lambda)\\) is the intensity of light entering, and \\(k_{s}L\\) is the optical depth, denoted \\(\\tau\\). When both scattering and absorption occur, the overall extinction can be defined as \\(k_{e}=k_{s}+k_{a}\\) for absorption coefficient \\(k_{a}\\). Extinction (or attenuation) is the sum of scattering and absorption, so it represents. total effect of medium on radiation passing the medium. The intensity is then can be found again, using the Beer-Lambert law as above: \\[\nI(\\lambda)=I_{0}(\\lambda)\\exp(-k_{e}L)\n\\]\n\n\n\nIllustration of a very rundown, average spectroscopic system.\n\n\nEven with some of its drawback, characteristic scattering is less susceptible to the environment (fluorescence spectroscopy), spectrum is easier to decodes (IR spectroscopy), somewhat easier to set up and maintain (reflection-absorption spectroscopy), and others."
  },
  {
    "objectID": "content/r29.html#the-problem",
    "href": "content/r29.html#the-problem",
    "title": "",
    "section": "The problem",
    "text": "The problem\nThe main problem concerned with this is that we have no conclusive, cohesive picture about the general theory of Raman scattering, spectroscopy, or even vibrational spectroscopy - the type of spectroscopy that Raman is based on, as of date. It is wise to note that since its conception by C. V. Raman, the theory can be abhorrently and arrogantly explained by classical physics, through a variety of ideas and layers, up to quantum physics of quantized phonon interaction between matters and so on. Nevertheless, it remains true, and rings true of its purpose, that there exists no cohesive picture, or a partially clear one that clears out the historical interpretation, models, and different approach leading to said problem. That is what the research is about. We note that depends on the timescale of the project, it might, or might not be exhaustive. Nevertheless, it is fair to say we still want at least a preliminary picture in general, of the topic itself."
  },
  {
    "objectID": "contact.html",
    "href": "contact.html",
    "title": "Contact",
    "section": "",
    "text": "This website is just the front of the research effort. For information regarding me, please refer to this website, where both my personal blog and CV site can be found.\n\n\n\n Back to top"
  },
  {
    "objectID": "citing.html",
    "href": "citing.html",
    "title": "Citing",
    "section": "",
    "text": "Well, what is even here to cite? Hopefully in the future.\n\n\n\n Back to top"
  },
  {
    "objectID": "content/r15.html",
    "href": "content/r15.html",
    "title": "Unattainable AGI and why it is not here",
    "section": "",
    "text": "For a long time, there exists the debate on whether the world is approaching the fundamental point of “Singularity”, as per the creation of an artificial general intelligence (AGI) in various forms, both in popular social phenomena and in research society. Just how much is this true?\n\nThe research is in progress, so don’t ask me why (草)\n\n\nSummary\nLarge Language Model, or LLM ((Vaswani et al. 2017),(Devlin et al. 2019),(Brown et al. 2020),(Zhao et al. 2023),(Zhao et al. 2024),(Radford et al. 2019),(Radford et al. 2018),(Raffel et al. 2020),(Touvron, Lavril, Izacard, Martinet, Lachaux, Lacroix, Rozière, Goyal, Hambro, Azhar, and others 2023),(Touvron, Martin, et al. 2023),(Aakanksha Chowdhery et al. 2022),(Ouyang et al. 2022),(Wei et al. 2022),(Kaplan et al. 2020),(Hoffmann et al. 2022),(Bai et al. 2022)) is one of the most successful, most advanced, and most developed type of model in the current modern machine learning landscape, and of AI (Artificial Intelligence) research at large. Its success has not been lacking, and its reputation and widespread uses have been proved over time. The effect of LLM has been realized, and indeed has been changing the landscape of society in a very much difficult way. Latest model of such architecture, like (OpenAI 2025b),(Wang et al. 2025),(OpenAI 2025a)’s GPT-5, (Guo et al. 2025),(DeepSeek AI / Hugging Face 2025)’s DeepSeek AI, (Anthropic 2024),(Anthropic Research 2025)’s Claude AI, (Touvron, Lavril, Izacard, Martinet, Lachaux, Lacroix, Rozière, Goyal, Hambro, Azhar, Rodriguez, et al. 2023)’s LLaMA, (Amitabh Chowdhery et al. 2022),(Anil et al. 2023)’s PaLM / PaLM-2, and (Jiang et al. 2023),(Mistral AI 2023)’s Mistral (Mistral-7B), pushed this boundary further and further, and levelling up many tasks and purposes with AI system in practice, and further onward with techniques like (Wei et al. 2022)’s Chain-of-Thought prompting, (Kaplan et al. 2020)’s scaling law, and more ((Hoffmann et al. 2022),(Bai et al. 2022)).\nHowever, with such development, come great expectation, great speculation, and also great hallucination. New development of the field of AI even earlier than paper on the Transformer neural network which fuelled the revolution of AI exposure, has gathered a group of people speculated about the further exponential growth of AI, almost to a degree of religious, about the topic of a Singularity, where AI will become Artificial General Intelligence (AGI). This is reflected in popular culture phenomena, speculation, researches, interpretation of reasoning behaviours and so on, for example, in (Barrat 2013),(Birch 2024),(Yudkowsky and Soares 2025),(Hao 2025),(Bostrom 2014), and more broadly on LLM in specific, (Mumuni and Mumuni 2025),(Shang et al. 2024),(David Ilić 2024),(Goertzel 2023),(Feng et al. 2024),(Ryunosuke Ishizaki 2025) and (Cui et al. 2024). The claim is clear - we are pushing toward the age of AGI, and perhaps sooner or later, reach the state of Artificial Superintelligence (ASI) of which cited in popular culture as the cultivation point of the Singularity - the shift of society toward a society of abundance, post-scarcity state. Most proponents point to LLM for such advancement, as it is one of the most widespread success and accessible form of interaction with AI systems on large scale. Pushbacks against such movement, including such as (Friedman 2024),(Anonymous 2024c),(Bender et al. 2021),(Baan 2021),(Anonymous 2023) on the “Stochastic Parrot Hypothesis”, (Anonymous 2024a) questioned of LLM path to AGI, the reverend (Forum 2025) post itself, (Anonymous 2024b),(Anonymous 2025) on generating suggestive limitation of research paper, (Marcus 2025),(Marcus 2023)‘s’ critique on generative AI on world models and failure of LLM, Sandra Johnson (2024) on limitation of LLM, similarly (Zhang et al. 2025), mathematics critique in (Mirzadeh et al. 2024), and more. However, the generally public, and more so of the positivity of the inner market on AI focus on the development and increment of larger models toward such goal. It is not too offset to hear the phrase “AGI will be in \\(X\\) days/month/year”, as much as it is a social phenomena even in small or large circle. Objectively, such positivity is not without basis. Furthermore, it is rather with certain amount of irony that the research made use of AI itself, for reference taking purposes.\nNevertheless, a critical task can be given out of such argument and thorough development of the current debate. What can then be extrapolated from the ongoing dilemma? What has to do with the architecture, the consideration about AGI that is now turned into the debate of will LLM be AGI? How is our understanding of the concept of AI, AGI, and ASI in general? And of a sense, what will provide us a pathway toward such goal?\n\n\n\n\n\n Back to topReferences\n\nAnil, R. et al. 2023. “PaLM 2 Technical Report.” arXiv preprint arXiv:2305.10403. https://arxiv.org/abs/2305.10403.\n\n\nAnonymous. 2023. “Stochastic Parrots: A Novel Look at Large Language Models and Their Limitations.” Towards AI. https://towardsai.net/p/machine-learning/stochastic-parrots-a-novel-look-at-large-language-models-and-their-limitations.\n\n\n———. 2024a. “How Close Is AGI Actually? Why LLMs Alone Will Not Get Us to AGI.” New Jersey Innovation Institute. https://www.njii.com/2024/07/why-llms-alone-will-not-get-us-to-agi/.\n\n\n———. 2024b. “LimGen: Probing the LLMs for Generating Suggestive Limitations of Research Papers.” arXiv Preprint arXiv:2403.15529. https://arxiv.org/abs/2403.15529.\n\n\n———. 2024c. “The Stochastic Parrot Hypothesis Is Debatable for the Last Generation of LLMs.” https://www.lesswrong.com/posts/HxRjHq3QG8vcYy4yy/the-stochastic-parrot-hypothesis-is-debatable-for-the-last.\n\n\n———. 2025. “Can LLMs Identify Critical Limitations Within Scientific Research? A Systematic Evaluation on AI Research Papers.” arXiv Preprint arXiv:2507.02694. https://arxiv.org/abs/2507.02694.\n\n\nAnthropic. 2024. “The Claude 3 Model Family: Opus, Sonnet, Haiku (Model Card).” https://www-cdn.anthropic.com/.../Model_Card_Claude_3.pdf.\n\n\nAnthropic Research. 2025. “Tracing the Thoughts of a Large Language Model.” https://www.anthropic.com/research/tracing-thoughts-language-model.\n\n\nBaan, Joris. 2021. “The Slodderwetenschap (Sloppy Science) of Stochastic Parrots – a Plea for Science to NOT Take the Route Advocated by Gebru and Bender.” arXiv Preprint arXiv:2101.10098. https://arxiv.org/abs/2101.10098.\n\n\nBai, Yuntao, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, et al. 2022. “Constitutional AI: Harmlessness from AI Feedback.” arXiv Preprint arXiv:2212.08073. https://arxiv.org/abs/2212.08073.\n\n\nBarrat, James. 2013. Our Final Invention: Artificial Intelligence and the End of the Human Era. Thomas Dunne Books.\n\n\nBender, Emily M, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. 2021. “On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?” In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, 610–23. ACM. https://doi.org/10.1145/3442188.3445922.\n\n\nBirch, Jonathan. 2024. The Edge of Sentience: Risk and Precaution in Humans, Other Animals, and AI. Oxford University Press.\n\n\nBostrom, Nick. 2014. Superintelligence: Paths, Dangers, Strategies. Oxford University Press.\n\n\nBrown, Tom, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, et al. 2020. “Language Models Are Few-Shot Learners.” In Advances in Neural Information Processing Systems, 33:1877–1901. Neural Information Processing Systems. https://arxiv.org/abs/2005.14165.\n\n\nChowdhery, Aakanksha, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, et al. 2022. “PaLM: Scaling Language Modeling with Pathways.” arXiv Preprint arXiv:2204.02311. https://arxiv.org/abs/2204.02311.\n\n\nChowdhery, Amitabh et al. 2022. “PaLM: Scaling Language Modeling with Pathways.” In. arXiv preprint arXiv:2204.02311. https://arxiv.org/abs/2204.02311.\n\n\nCui, Tianyu, Yanling Wang, Chuanpu Fu, Yong Xiao, Sijia Li, Xinhao Deng, Yunpeng Liu, et al. 2024. “Risk Taxonomy, Mitigation, and Assessment Benchmarks of Large Language Model Systems.” arXiv Preprint. https://arxiv.org/abs/2401.05778.\n\n\nDavid Ilić, Gilles E. Gignac. 2024. “Evidence of Interrelated Cognitive-Like Capabilities in Large Language Models: Indications of Artificial General Intelligence or Achievement?” Intelligence 106: 101858. https://doi.org/10.1016/j.intell.2024.101858.\n\n\nDeepSeek AI / Hugging Face. 2025. “Deepseek-Ai/DeepSeek-R1 (Model Card).” https://huggingface.co/deepseek-ai/DeepSeek-R1.\n\n\nDevlin, Jacob, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. “BERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding.” In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), 4171–86. Association for Computational Linguistics. https://arxiv.org/abs/1810.04805.\n\n\nFeng, Tao, Chuanyang Jin, Jingyu Liu, Kunlun Zhu, Haoqin Tu, Zirui Cheng, Guanyu Lin, and Jiaxuan You. 2024. “How Far Are We from AGI: Are LLMs All We Need?” arXiv Preprint arXiv:2405.10313, May. https://arxiv.org/abs/2405.10313.\n\n\nForum, AI Alignment. 2025. “Beware General Claims about \"Generalizable Reasoning Capabilities\" (of Modern AI Systems).” AI Alignment Forum. https://www.alignmentforum.org/posts/5uw26uDdFbFQgKzih/beware-general-claims-about-generalizable-reasoning.\n\n\nFriedman, Dave. 2024. “Understanding Inference and the \"Stochastic Parrot\" in Large Language Models.” Personal blog (Substack). https://davefriedman.substack.com/p/understanding-inference-and-the-stochastic.\n\n\nGoertzel, Ben. 2023. “Generative AI Vs. AGI: The Cognitive Strengths and Weaknesses of Modern LLMs.” arXiv Preprint. https://arxiv.org/abs/2309.10371.\n\n\nGuo, D. et al. 2025. “DeepSeek-R1: Incentivizing Reasoning Capability in LLMs.” arXiv preprint arXiv:2501.12948. https://arxiv.org/abs/2501.12948.\n\n\nHao, Karen. 2025. Empire of AI: Dreams and Nightmares in Sam Altman’s OpenAI. Penguin Press.\n\n\nHoffmann, Jordan, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, et al. 2022. “Training Compute-Optimal Large Language Models.” arXiv Preprint arXiv:2203.15556. https://arxiv.org/abs/2203.15556.\n\n\nJiang, A. Q. et al. 2023. “Mistral 7B.” arXiv preprint arXiv:2310.06825. https://arxiv.org/abs/2310.06825.\n\n\nKaplan, Jared, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. 2020. “Scaling Laws for Neural Language Models.” arXiv Preprint arXiv:2001.08361. https://arxiv.org/abs/2001.08361.\n\n\nMarcus, Gary. 2023. “Elegant and Powerful New Result That Seriously Undermines Large Language Models.” Marcus on AI (Substack). https://garymarcus.substack.com/p/elegant-and-powerful-new-result-that.\n\n\n———. 2025. “A Knockout Blow for LLMs?” Communications of the ACM. https://cacm.acm.org/blogcacm/a-knockout-blow-for-llms/.\n\n\nMirzadeh, Iman, Keivan Alizadeh, Hooman Shahrokhi, Oncel Tuzel, Samy Bengio, and Mehrdad Farajtabar. 2024. “GSM-Symbolic: Understanding the Limitations of Mathematical Reasoning in Large Language Models.” arXiv Preprint arXiv:2410.05229. https://arxiv.org/abs/2410.05229.\n\n\nMistral AI. 2023. “Announcing Mistral 7B.” https://mistral.ai/news/announcing-mistral-7b.\n\n\nMumuni, Alhassan, and Fuseini Mumuni. 2025. “Large Language Models for Artificial General Intelligence: A Survey of Foundational Principles and Approaches.” arXiv Preprint. https://arxiv.org/abs/2501.03151.\n\n\nOpenAI. 2025a. “Inside GPT-5 for Work.” https://cdn.openai.com/pdf/inside-gpt-5-for-work.pdf.\n\n\n———. 2025b. “Introducing GPT-5.” https://openai.com/gpt-5/.\n\n\nOuyang, Long, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, et al. 2022. “Training Language Models to Follow Instructions with Human Feedback.” Advances in Neural Information Processing Systems 35: 27730–44. https://arxiv.org/abs/2203.02155.\n\n\nRadford, Alec, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018. “Improving Language Understanding by Generative Pre-Training.” OpenAI Blog. https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf.\n\n\nRadford, Alec, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. “Language Models Are Unsupervised Multitask Learners.” OpenAI Blog 1 (8): 9. https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf.\n\n\nRaffel, Colin, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. “Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer.” Journal of Machine Learning Research 21 (140): 1–67. https://arxiv.org/abs/1910.10683.\n\n\nRyunosuke Ishizaki, Mahito Sugiyama. 2025. “Large Language Models: Assessment for Singularity.” AI & Society. https://link.springer.com/article/10.1007/s00146-025-02271-4.\n\n\nSandra Johnson, David Hyland-Wood. 2024. “A Primer on Large Language Models and Their Limitations.” arXiv Preprint arXiv:2412.04503. https://arxiv.org/abs/2412.04503.\n\n\nShang, Jingbo, Zai Zheng, Jiale Wei, Xiang Ying, Felix Tao, and Mindverse Team. 2024. “AI-Native Memory: A Pathway from LLMs Towards AGI.” arXiv Preprint. https://arxiv.org/abs/2406.18312.\n\n\nTouvron, Hugo, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, and others. 2023. “LLaMA: Open and Efficient Foundation Language Models.” arXiv Preprint arXiv:2302.13971. https://arxiv.org/abs/2302.13971.\n\n\nTouvron, Hugo, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurélien Rodriguez, et al. 2023. “LLaMA: Open and Efficient Foundation Language Models.” arXiv Preprint arXiv:2302.13971. https://arxiv.org/abs/2302.13971.\n\n\nTouvron, Hugo, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, et al. 2023. “Llama 2: Open Foundation and Fine-Tuned Chat Models.” arXiv Preprint arXiv:2307.09288. https://arxiv.org/abs/2307.09288.\n\n\nVaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. “Attention Is All You Need.” In Advances in Neural Information Processing Systems, 30:5998–6008. Neural Information Processing Systems. https://arxiv.org/abs/1706.03762.\n\n\nWang, Shansong, Mingzhe Hu, Qiang Li, Mojtaba Safari, and Xiaofeng Yang. 2025. “Capabilities of GPT-5 on Multimodal Medical Reasoning.” https://arxiv.org/abs/2508.08224.\n\n\nWei, Jason, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022. “Chain-of-Thought Prompting Elicits Reasoning in Large Language Models.” Advances in Neural Information Processing Systems 35: 24824–37. https://arxiv.org/abs/2201.11903.\n\n\nYudkowsky, Eliezer, and Nate Soares. 2025. If Anyone Builds It, Everyone Dies: Why Superhuman AI Would Kill Us All. Hachette Book Group.\n\n\nZhang, Yijun et al. 2025. “LLLMs: A Data-Driven Survey of Evolving Research on Limitations of Large Language Models.” arXiv Preprint arXiv:2505.19240, May. https://arxiv.org/abs/2505.19240.\n\n\nZhao, Wayne Xin et al. 2024. “Large Language Models: A Survey.” arXiv Preprint arXiv:2402.06196. https://arxiv.org/abs/2402.06196.\n\n\nZhao, Wayne Xin, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, et al. 2023. “A Survey of Large Language Models.” arXiv Preprint arXiv:2303.18223. https://arxiv.org/abs/2303.18223."
  },
  {
    "objectID": "faq.html",
    "href": "faq.html",
    "title": "FAQ",
    "section": "",
    "text": "Are there even anything to say? I mean yes, but yeah, generaly, it is not rigorous enough to warrant a direct affiliation toward what to ask, even.\n\n\n\n Back to top"
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "",
    "section": "",
    "text": "This page includes the list of projects, researches in questions, proposals and ideas that is of consideration in the future. Most of them are incomplete, and are welcoming adoption from certain collaboration if requested or encouraged. While this is not always up-to-date (as I have an internal organization structure), it should be, most of the time adequate. For now, let us construct the main foundation on what are we going to do with the short-term papers and not the long-term, 2.5 years waitlist papers."
  },
  {
    "objectID": "publications.html#current-works",
    "href": "publications.html#current-works",
    "title": "",
    "section": "Current works",
    "text": "Current works\nCurrently, the work that I am working on is:\n\nNeural network documentation and sources (here). Progress is kind of slow, but is expected.\nNeuroscience documentation (knowledge base) project. (Currently the website html template is having issues, so I am putting that on hold for now).\nQuantum physics blog (for now the website is in preparation, but will use Hugo template so I have to familiarize myself with it).\nA paper on Raman spectroscopy and ML application on yogurt (organize in this sheet).\nA paper on Quantum well analysis of nonlinear absorption coefficient - will have to explain this in a few separated documents and thereof that is. Work alongside Duong Ngoc Khoa.\nA paper on Double descent in theoretical machine learning and potential analysis on such. It is a landmark paper in the sense of efforts poured in."
  },
  {
    "objectID": "publications.html#list-of-interesting-topics-on-holds",
    "href": "publications.html#list-of-interesting-topics-on-holds",
    "title": "",
    "section": "List of interesting topics on holds",
    "text": "List of interesting topics on holds\nFor now, here are some of the paper topics:\n\nDouble descent: a review (review, reference): A reference and review paper talking about double descent. Including effort to formalize the definition of it.\nInterpreting double descent in polynomial via density estimation (original, theory): A fairly light and theoretical paper on the toy model description of polynomial model and different interpretation.\nDouble descent on support vector machine and polynomial models on binary classification - an analysis (experimental, theory): A both theory and experimental paper.\nPINNs and double descent (experimental): An experimental on PINNs and how to identify double descent on it.\nClassical learning theory and future (original, review, experiments).\nA review of model structures in machine learning and theoretical machine learning (theoretical, review, reference): A reference paper on the concept of models in machine learning, their structures, flavour, consideration, and thereof.\nDifferent treatment of machine learning - an analysis (review, reference, original): A paper on the different treatment of machine learning, from different lens and scale, and how do they fit together (categorical machine learning for example).\nReuniting the neural network framework using partial category theory (original, theory): Using the proposed categorical machine learning or neural network to consider reuniting different architecture together under the same framework.\nOn the capacity and capability of neural networks (review, original): A paper analysing the capacity and capability of neural networks on various tasks and setting thereof, and so forth, considering different structures.\nMachine learning from a mathematical modelling view (review, original): Proposing a modelling theoretic, internal state interpretation to machine learning.\nThe analysis of statistical physics on machine learning (review, original): Setting up a review work on how statistical physics is related to machine learning, where is it failing and what constitute the failure.\nClassical model into neural network architecture - an analysis (original, theoretical): An attempt to attack and formalize classical models into neural network theoretic (unit-wise approach of neuronal unit) to classical models, hence gauging their capability.\nUnsolved problems in theoretical machine learning (original, review): A work in review of different interpretations and problems residing the current framework of machine learning.\nDifferential equations and machine learning - interpreting machine learning system using differential equations (original, theory): An attempt to express ML systems in terms of differential equations, just as the differential model.\nLLM cannot be AGI - the kind of paper. Main page for this is here.\nNeural network learning equals mathematical model structural approximation (original, theory, experiment).\nConcentration inequalities in theoretical machine learning for beginner.\nStructural addition in machine learning - an analysis.\nComponent and structural realization of large language model and why they are not intelligent - just to prove that LLM is not, well, intelligent and again, won’t ever be AGI.\nHallucination is bias-variance tradeoff - we just, well, connect it to them.\nVisualization of neural network operation and layer theoretic.\nTime-sensitive processing network - something that look like an operational system in the inner units of the neural network, so yeah.\nAlways running neural network. Mimicking a network to well, always run and is not static.\nVisualizing dynamics and bias-variance plus double descent.\nInformation theoretic interpretation of machine learning framework.\nRandomized neural network architecture on permutations (theory, experiment, original) - just a way to do such analysis.\nMultiNet - A new multidirectional structure of neural network formalism (original, experimentation) - just a generalization of what I observed of neural network structure in general. Will have to base on existing facility, can’t do anything else.\nTheoretical explanation for Neural Scaling Law (original, theoretical) - Just an attempt to solve the neural scaling law.\n\nFor now, they are short papers for machine learning. On physics, well.\n\nThe theory of Raman Spectroscopy (review, reference): On the theory of Raman spectroscopy on itself, different interpretation, history, approaches, and problems. Research page is this\n\nThat is it for now. More is in the PDF file itself, which is listed there for reasons."
  },
  {
    "objectID": "tutorial.html",
    "href": "tutorial.html",
    "title": "Tutorial for documentation",
    "section": "",
    "text": "Since a lot of my work is on the website, which is deliberately built upon quarto and thereof, there are tutorials to be made in advance. Some of these tutorials are from NOAA webpages."
  },
  {
    "objectID": "tutorial.html#edit-and-add-your-pages",
    "href": "tutorial.html#edit-and-add-your-pages",
    "title": "Tutorial for documentation",
    "section": "Edit and add your pages",
    "text": "Edit and add your pages\nEdit the qmd or md files in the content folder. qmd files can include code (R, Python, Julia) and lots of Quarto markdown bells and whistles (like call-outs, cross-references, auto-citations and much more).\nEach page should start with\n---\ntitle: your title\n---\nand the first header will be the 2nd level, so ##. Note, there are situations where you leave off\n---\ntitle: your title\n---\nand start the qmd file with a level header #, but if using the default title yaml (in the --- fence) is a good habit since it makes it easy for Quarto convert your qmd file to other formats (like into a presentation)."
  },
  {
    "objectID": "tutorial.html#add-your-pages-the-project",
    "href": "tutorial.html#add-your-pages-the-project",
    "title": "Tutorial for documentation",
    "section": "Add your pages the project",
    "text": "Add your pages the project\n\nAdd the files to _quarto.yml"
  },
  {
    "objectID": "tutorial.html#webpage-publishing",
    "href": "tutorial.html#webpage-publishing",
    "title": "Tutorial for documentation",
    "section": "Webpage publishing",
    "text": "Webpage publishing\nTo get your Quarto webpage to show up with the url\nyourname.github.io/yourrepo\nyou have a few steps.\n\nTurn on GitHub Pages for your repo\n\nTurn on GitHub Pages under Settings &gt; Pages . You will set pages to be made from the gh-pages branch and the root directory.\nTurn on GitHub Actions under Settings &gt; Actions &gt; General\n\nThe GitHub Action will automatically recreate your website when you push to GitHub after you do the initial gh-pages set-up\n\n\nDo your first publish\nThe first time you publish to gh-pages, you need to do so locally.\n\nOn your local computer, open a terminal window and cd to your repo directory. Here is what that cd command looks like for me. You command will look different because your local repo will be somewhere else on your computer.\n\ncd ~/Documents/GitHub/NOAA-quarto-simple\n\nPublish to the gh-pages. In the terminal type\n\nquarto publish gh-pages\nThis is going to render your webpage and then push the _site contents to the gh-pages branch, which is also most of the time the website is deployed."
  }
]